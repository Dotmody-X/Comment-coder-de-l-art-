
## Gan - Génération de cartoons

Pendant que je bossais à fond sur Visual Studio, j’avais besoin, en parallèle, d’un projet plus court, plus “tech test”, où je pouvais continuer à coder, mais sans forcément m’embarquer dans une interface ou une logique ultra complexe. C’est comme ça que j’ai lancé un petit défi en lien avec quelque chose que j’avais remarqué dans beaucoup d’affiches électro, rave, etc. : on y voit souvent des illustrations en style cartoon(imagenote: content/images/13_3.jpg caption: Affiche de référence). Des persos absurdes, naïfs, avec un côté à la fois trash et drôle. Et je me suis dit : “Pourquoi est-ce qu’on ne générerait pas ces trucs-là automatiquement ?”

<br>

Toujours dans la logique de mon TFE, ça allait dans le sens de ce que je cherchais : créer avec du code des outils ou des mécanismes qui permettent de gagner du temps, d’élargir le champ des possibles, de proposer des variations, et d’ouvrir des portes aux créateurs. Même si je savais que ce serait imparfait, j’avais envie de tenter l’entraînement d’un GAN (réseau antagoniste génératif) sur un dataset maison d’illustrations cartoon.

>C’est comme ça qu’est né ce mini-projet de GAN.

### Préparation des données

J’ai commencé par rassembler un dataset assez simple de visuels en style cartoon(imagenote: content/images/13_2.jpg caption: Database). Comme je voulais avoir un maximum de diversité, j’ai constitué ma propre base de données, avec environ 2000 images en noir et blanc et 2000 images en couleur(imagenote: content/images/13_1.png caption: Database). Ça m’a demandé pas mal de temps de tri, de nettoyage, de renommage de fichiers et de vérification, mais c’était indispensable pour avoir une base cohérente et exploitable.

<br>

Pour pouvoir exploiter tout ça avec PyTorch, j’ai ensuite codé un dataset_loader.py qui applique toutes les transformations nécessaires aux images : redimensionnement, recadrage, et normalisation dans une plage standard (de -1 à 1)

<br class="breakpage">

#### **_Code.py_**

<div class="codepde">
# dataset_loader.py<br>
transform = transforms.Compose([<br>
    transforms.Resize(image_size),<br>
    transforms.CenterCrop(image_size),<br>
    transforms.ToTensor(),<br>
    transforms.Normalize((0.5,), (0.5,))<br>
])<br>

dataset = datasets.ImageFolder(root=dataroot, transform=transform)
</div>

>Pour l'instant, juste je nettoie, je prépare les données, et je m’assure que chaque image sera lue correctement par le GAN pendant l’entraînement. En gros, facile !

### Créer le modèle

Ensuite, j’ai construit le cœur du GAN dans model.py : un générateur qui part de bruit aléatoire pour tenter de créer des images, et un discriminateur qui évalue si le résultat est réaliste ou non.(imagenote: content/images/13_4.png caption: Samples 50 du GAN)

#### **_Code.py_**

<div class="codepde">
# model.py (extrait du générateur)<br>
self.main = nn.Sequential(<br>
    nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),<br>
    nn.BatchNorm2d(ngf * 8),<br>
    nn.ReLU(True),<br>
    […]<br>
    nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),<br>
    nn.Tanh()<br>
)
</div>

<br>

Le discriminateur, lui, fait l’inverse : il prend une image en entrée et essaie de deviner si elle est vraie ou générée. Le système apprend par confrontation.

### Lancer l’entraînement

Une fois les deux réseaux définis, j’ai codé le train.py pour organiser l’entraînement, sauvegarder les modèles à chaque étape, afficher les progrès (ou les échecs), et monitorer les pertes.

#### **_Code.py_**

<div class="codepde">
# train.py (extrait)<br>
for epoch in range(num_epochs):<br>
    for i, data in enumerate(dataloader, 0):<br>
        # 1. Train Discriminator<br>
        netD.zero_grad()<br>
        # Données réelles<br>
        real = data[0].to(device)<br>
        label.fill_(real_label)<br>
        output = netD(real).view(-1)<br>
        errD_real = criterion(output, label)<br>
        errD_real.backward()<br>
        <br>
        # Données générées<br>
        noise = torch.randn(b_size, nz, 1, 1, device=device)<br>
        fake = netG(noise)<br>
        label.fill_(fake_label)
        output = netD(fake.detach()).view(-1)<br>
        errD_fake = criterion(output, label)<br>
        errD_fake.backward()<br>
<br>
        # Optimiser les deux<br>
        optimizerD.step()<br>
        optimizerG.step()
</div>

>Et c’est là que les choses sont devenues moins simples.

### Des résultats… moyens

Je vais pas te mentir : le résultat était décevant. Le GAN a réussi à produire des trucs… mais c’était flou, très bruité, sans vraiment de forme reconnaissable. Je pense que mon générateur était trop limité, ou alors que le discriminateur était trop sévère. Ou peut-être que les deux s’annulaient. En tout cas, les visuels n’étaient pas à la hauteur de ce que j’espérais. Quelques formes intéressantes sont apparues, mais rien d’exploitable pour une affiche ou une base visuelle.(imagenote: content/images/13_5.png caption: Samples 3500 du GAN)(imagenote: content/images/13_6.png caption: Samples 4800 du GAN)(imagenote: content/images/13_7.png caption: Samples 5000 du GAN)

### Mes enseignements

Même si ce n’est pas un projet que je vais utiliser tel quel, il m’a énormément appris. Déjà, il m’a mis un pied dans le monde du deep learning, et j’ai compris comment fonctionnent les GAN, les limites des jeux de données, les subtilités des pertes, etc. J’ai aussi vu à quel point l’IA peut devenir un outil complémentaire à la création graphique, même si ça demande du travail, du matos et du temps.

<br>

Ce projet n’était pas censé donner des chefs-d’œuvre, mais juste tester un autre moyen de produire. Et même si le résultat est foiré, la démarche tient la route, et je sais qu’un jour, en reprenant ça avec plus de puissance et un dataset mieux pensé, je pourrais en tirer quelque chose de fort.

>**Temps total du projet : 8h30.**

---
